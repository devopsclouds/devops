pipepline workflow and stages.
so when it comes to my pipeline we use git flow strategy here when developers be working on their features branches  and merges  it to the developer branch.

# multistage build: its used to optimize the container images and make them smaller.
multi-stage builds let you create optimized Docker images with only the dependencies necessary to build your application. Combined with Dockerâ€™s layered images, this can help you save significant space. The multi-stage process saves space on your Docker host and in the Docker image and speeds up the build process
This way, you can separate the build and runtime environments in the same Dockerfile. Use build environment as a dependency [COPY --from=build HelloWorld.class .] while creating the Dockerfile with the approach of multi-stage docker build. This will help minimize the size of Docker images.

What is Docker daemon?
Docker daemon runs on the host operating system. It is responsible for running containers to manage docker services. Docker daemon communicates with other daemons. It offers various Docker objects such as images, containers, networking, and storage. 

Docker architecture
Docker follows Client-Server architecture, which includes the three main components that are Docker Client, Docker Host, and Docker Registry.

On typically docker send a commands to docker daemon by using API. Docker daemon sends to local registry  for docker images.local registrt is a place to store the images. If the image is find on local registry it will run the container for that image.
Remote registry: all the images are verrifyed and its more secure

Send the request to docker daemon
Then docker daemon uses containerd containerd uses runc  and runc to start the containers.

docker build
docker pull
docker run
2. Docker Host
Docker Host is used to provide an environment to execute and run applications. It contains the docker daemon, images, containers, networks, and storage.
3. Docker Registry
Docker Registry manages and stores the Docker images.
There are two types of registries in the Docker -
Pubic Registry - Public Registry is also called as Docker hub.
Private Registry - It is used to share images within the enterprise.
Docker Objects
There are the following Docker Objects -
Docker Images
Docker images are the read-only binary templates used to create Docker Containers. It uses a private container registry to share container images within the enterprise and also uses public container registry to share container images within the whole world. Metadata is also used by docket images to describe the container's abilities.
Docker Containers
Containers are the structural units of Docker, which is used to hold the entire package that is needed to run the application. The advantage of containers is that it requires very less resources.
In other words, we can say that the image is a template, and the container is a copy of that template.
 
Docker Networking
Using Docker Networking, an isolated package can be communicated. Docker contains the following network drivers -
o	Bridge - Bridge is a default network driver for the container. It is used when multiple docker communicates with the same docker host.
o	Host - It is used when we don't need for network isolation between the container and the host.
o	None - It disables all the networking.
o	Overlay - Overlay offers Swarm services to communicate with each other. It enables containers to run on the different docker host.
o	Macvlan - Macvlan is used when we want to assign MAC addresses to the containers.
Docker Storage
Docker Storage is used to store data on the container. Docker offers the following options for the Storage -
o	Data Volume - Data Volume provides the ability to create persistence storage. It also allows us to name volumes, list volumes, and containers associates with the volumes.
o	Directory Mounts - It is one of the best options for docker storage. It mounts a host's directory into a container.
o	Storage Plugins - It provides an ability to connect to external storage platforms.
Docker vs Virtual Machine
The following section aims to discuss the difference between Docker and VM:
Docker vs VM: Architecture


 
In the vm or docker start with physical server it can be any server
On hypervisior  it could be vmware 
Then vmware spin up the vms 
And each vm dedicate certain amount of resources might be 4GB Ram
And each vm consilodated some amount of space for guest os . you need to allocate a some amount of resource for guest os . if guest os might be windows machine u need lience version

If u are deploying the multiple applications u need multiple vms 
In docker it allows the multiple containers . All container utilizes the same operating system .docker engine emulates the file system ,process id for the container and not emulates the operating system 

All docker docker are run on docker hosts that all are isolated . docker containers are not interface from one container to another container


#microservices
Before microservices a monolthitic was a standard
let take the example of online shopping like user login , items , add to cart,payment
All these components are part of a single unit and all these functionalities in one code base
everthing is developed , deployed and scaled as a 1 unit.
App must be written with 1 tech stack.
Team  need to be careful to not affect with each other work.
1 single artifact you must redeploy the entire application on each update.


challenges of monolithic 
Application is too large and complex
parts are more tangled into each other
you can only scale the entire app instead of a specific service
high infrastracture costs
Release process takes longer
on every change the entire application need to be tested
 entire application need to be built and deployed
 bug in any module can potentinal bring down the entire application.
 
 
 
 # microservices
 
 to break the application based on functionalities
 each microservices can choose its own tech stack.
 each service has self contained and implements a single bussiness capability
 each team can develop the services independently without affecting it others
 There is a change in one of the component then you need to deploy the particular component
 
 communciation between one component to another component by using istio service mesh.
 
 Downsides of microservices
 configure the communcation between services
 
 mono repo vs poly repo
 
 ##istio
 istio: istio is a service mesh. service mesh manages communication between micro services.
service mesh with sidecar pattern
  sidecar proxy handles these networking logic and act as a proxy and it is third-party application . cluster operations can configure it easily . developer can focus on the actual bussiness logic. control plane injects the side car proxy in every micro service. now the microservices can talk to each using proxy.
  you dont have to adjust deployment and service k8s yamls
  istio configuration seperate from actual configuration.
  
  on of the most important feature in istio service mesh is traffic splitting. if there is any change in one of the service is like payment service and new version it will build , tested and then deploy to production . you cant be sure ther is a no bug . in this case you can send the traffic 10 percent for new versions.release new versions without worrying about breaking the application.
  

  
  
 
 mono repo has one repo that contains multiple services in git hub or git lab
 poly repo: each service has own repoistory
 ## kubernetes arc:
 1.worker nodes are nothing but virtual machines or physcial servers running within in a data centre . All these nodes join together

to form a cluster.
2.pod is scheduling unit in kubernetes . each pod consists of one or more containers , most cases only one container. container are
run run time enviornment for run time appilication . containers are designed to run microservices application are not monolithic application

3.master is resposible for manging the entire cluster.it monitors the health check of the nodes.when the work node fails it moves work node to failure node.
and keep to  another healthy node

k8s master components
API Server: Api server is the entire gatekeeper of cluster. 
API : all the objects perform in the API like pod craetion, service and any operation , API using kubectl command or UI
then Api interacts with api server. api server should do the validates the objects.

then api server sends to schedular
shecudelar . shecudlig in the pods acrosss multipe nodes   kubernetes deponds upon the constraints like memory etc
suppose if the application requires x constarints the scheduler will look the appromaite work node for that application

controller manager --- detects control cluster state changes
they are four
1.node point controller
2.repplication controller
3.end point controller

these controller are responsible for health check of entire cluster
it ensures the nodes are up and running
it ensure the correct number of pods are running as mention in the spec file

etcd: etcd is central data base store current state of the cluster at any point of time. it is  key value distributed data base the data will stored related to cluster
worker node components
kubelet : kubelet is the primary node agent in every worker node inside the cluster.kubelet interacts with the both container and node.kubelet start the pod with a 
container inside. And assign resources from node to the container.bascially it look the pod spec and submitted to api server in master
ensure that pod to be running and healthy.if the kubelet notices any issues in the pod in the worker node . then it will restart the pods in same worker node
if the problem within in the worker node itself then the matser detects the failure node and launches the pod in another worker node.
if all depends on pod is replication controller or repplication set

kube proxy . It is responsible for mantaining the entire network configuration and it also exposes the service to outside the world


every node have multiple pods associated with it
worker node do the actual work
Every node is unique is ip adddress . in the kubernetes when we deployed the pod in the cluster we get pod ip address



https://www.salesforceben.com/wp-content/uploads/2021/06/workflow3-768x476.png

At the start of the sprint, you create a release branch from the master/main branch.
Feature branches are created by each member of your team as they begin work on each user story.
You then build your changes in individual dev sandboxes (and/or IDE) and periodically commit them to the feature branch.
When your feature is ready for QA, you open a pull request and merge the feature branch into the QA integration branch.
After merging any change to QA integration, your branch gets deployed to the QA sandbox, where it can be tested and signed off by your QA team.
Once this is complete, a PR is opened to merge the feature branch into the Stage integration branch.
After merging any change to Stage integration, the branch is deployed to the Stage sandbox, where it can be tested and signed off for final release to production. Depending on your workflow, you might also have UAT by end users in there for good measure.
Once the change is approved in Stage, the feature branch is merged into the release branch. This process then repeats itself for every change made during the sprint, with each approved feature collected in the release branch.
At the end of the sprint, all changes in the release branch are deployed to production and set live for your end users.
Once youâ€™ve verified the new work on production, you merge the release branch back into master, so that master is once again up to date with production.
Youâ€™d then also want to merge master back into the integration branches, so that your branches stay in sync should any rogue changes have not followed this main process.
