method overloading
Method overloading is a means by which you can call the same method in different ways, i.e. with different parameters based on the number of arguments or their different datatypes. 
method overiding
When we call displayInfo() using the d1 object (object of the subclass), the method inside the subclass Dog is called. The displayInfo() method of the subclass overrides the same method of the superclass. 

pipepline workflow and stages.
so when it comes to my pipeline we use git flow strategy here when developers be working on their features branches  and merges  it to the developer branch.

# multistage build: its used to optimize the container images and make them smaller.
multi-stage builds let you create optimized Docker images with only the dependencies necessary to build your application. Combined with Dockerâ€™s layered images, this can help you save significant space. The multi-stage process saves space on your Docker host and in the Docker image and speeds up the build process
This way, you can separate the build and runtime environments in the same Dockerfile. Use build environment as a dependency [COPY --from=build HelloWorld.class .] while creating the Dockerfile with the approach of multi-stage docker build. This will help minimize the size of Docker images.

What is Docker daemon?
Docker daemon runs on the host operating system. It is responsible for running containers to manage docker services. Docker daemon communicates with other daemons. It offers various Docker objects such as images, containers, networking, and storage. 

Docker architecture
Docker follows Client-Server architecture, which includes the three main components that are Docker Client, Docker Host, and Docker Registry.

On typically docker send a commands to docker daemon by using API. Docker daemon sends to local registry  for docker images.local registrt is a place to store the images. If the image is find on local registry it will run the container for that image.
Remote registry: all the images are verrifyed and its more secure

Send the request to docker daemon
Then docker daemon uses containerd containerd uses runc  and runc to start the containers.

docker build
docker pull
docker run
2. Docker Host
Docker Host is used to provide an environment to execute and run applications. It contains the docker daemon, images, containers, networks, and storage.
3. Docker Registry
Docker Registry manages and stores the Docker images.
There are two types of registries in the Docker -
Pubic Registry - Public Registry is also called as Docker hub.
Private Registry - It is used to share images within the enterprise.
Docker Objects
There are the following Docker Objects -
Docker Images
Docker images are the read-only binary templates used to create Docker Containers. It uses a private container registry to share container images within the enterprise and also uses public container registry to share container images within the whole world. Metadata is also used by docket images to describe the container's abilities.
Docker Containers
Containers are the structural units of Docker, which is used to hold the entire package that is needed to run the application. The advantage of containers is that it requires very less resources.
In other words, we can say that the image is a template, and the container is a copy of that template.
 
Docker Networking
Using Docker Networking, an isolated package can be communicated. Docker contains the following network drivers -
o	Bridge - Bridge is a default network driver for the container. It is used when multiple docker communicates with the same docker host.
o	Host - It is used when we don't need for network isolation between the container and the host.
o	None - It disables all the networking.
o	Overlay - Overlay offers Swarm services to communicate with each other. It enables containers to run on the different docker host.
o	Macvlan - Macvlan is used when we want to assign MAC addresses to the containers.
Docker Storage
Docker Storage is used to store data on the container. Docker offers the following options for the Storage -
o	Data Volume - Data Volume provides the ability to create persistence storage. It also allows us to name volumes, list volumes, and containers associates with the volumes.
o	Directory Mounts - It is one of the best options for docker storage. It mounts a host's directory into a container.
o	Storage Plugins - It provides an ability to connect to external storage platforms.
Docker vs Virtual Machine
The following section aims to discuss the difference between Docker and VM:
Docker vs VM: Architecture


 
In the vm or docker start with physical server it can be any server
On hypervisior  it could be vmware 
Then vmware spin up the vms 
And each vm dedicate certain amount of resources might be 4GB Ram
And each vm consilodated some amount of space for guest os . you need to allocate a some amount of resource for guest os . if guest os might be windows machine u need lience version

If u are deploying the multiple applications u need multiple vms 
In docker it allows the multiple containers . All container utilizes the same operating system .docker engine emulates the file system ,process id for the container and not emulates the operating system 

All docker docker are run on docker hosts that all are isolated . docker containers are not interface from one container to another container


#microservices
Before microservices a monolthitic was a standard
let take the example of online shopping like user login , items , add to cart,payment
All these components are part of a single unit and all these functionalities in one code base
everthing is developed , deployed and scaled as a 1 unit.
App must be written with 1 tech stack.
Team  need to be careful to not affect with each other work.
1 single artifact you must redeploy the entire application on each update.


challenges of monolithic 
Application is too large and complex
parts are more tangled into each other
you can only scale the entire app instead of a specific service
high infrastracture costs
Release process takes longer
on every change the entire application need to be tested
 entire application need to be built and deployed
 bug in any module can potentinal bring down the entire application.
 
 
 
 # microservices
 
 to break the application based on functionalities
 each microservices can choose its own tech stack.
 each service has self contained and implements a single bussiness capability
 each team can develop the services independently without affecting it others
 There is a change in one of the component then you need to deploy the particular component
 
 communciation between one component to another component by using istio service mesh.
 
 Downsides of microservices
 configure the communcation between services
 
 mono repo vs poly repo
 
 challenges of microservice arch:
one microservice can talk to another serice inside the cluster it is insecure . soo developers have to add security logic in each microservice
and also communication logic and retry logic added in each microservice . for each microservieces monitoring  have to add promothesis or new relic in microservice.all these non bussiness logic added in each microservice along with bussiness logic.

solution is service mesh
istio: istio is a service mesh. service mesh manages communication between micro services.
service mesh with sidecar pattern
  sidecar proxy handles these networking logic and act as a proxy and it is third-party application . cluster operations can configure it easily . developer can focus on the actual bussiness logic. control plane injects the side car evoy proxy in every micro service. now the microservices can talk to each using proxy.
  you dont have to adjust deployment and service k8s yamls
  istio configuration seperate from actual configuration.
  istio is configured with k8s yams and uses CRD
  
  on of the most important feature in istio service mesh is traffic splitting. if there is any change in one of the service is like payment service and new version it will build , tested and then deploy to production . you cant be sure ther is a no bug . in this case you can send the traffic 10 percent for new versions.release new versions without worrying about breaking the application.
  
 second feature is traffic routing -- which services can communicate 
virtual service--- how you route traffic to a given destination

proxies can communicating without connecting to istiod

feature is service discovery:when new microservice is deployed is automatically created end points.istiod uses centeral registry in each microservice 
istiod uses cerificate management :secure tls communication between services
and finally provides metric for each micro service in envoy proxy

istio gateway: its load balancer it accepting incoming traffic inside the cluster from micro services


 
 mono repo has one repo that contains multiple services in git hub or git lab
 poly repo: each service has own repoistory
 ## kubernetes arc:
 1.worker nodes are nothing but virtual machines or physcial servers running within in a data centre . All these nodes join together

to form a cluster.
2.pod is scheduling unit in kubernetes . each pod consists of one or more containers , most cases only one container. container are
run run time enviornment for run time appilication . containers are designed to run microservices application are not monolithic application

3.master is resposible for manging the entire cluster.it monitors the health check of the nodes.when the work node fails it moves work node to failure node.
and keep to  another healthy node

k8s master components
API Server: Api server is the entire gatekeeper of cluster. 
API : all the objects perform in the API like pod craetion, service and any operation , API using kubectl command or UI
then Api interacts with api server. api server should do the validates the objects.

then api server sends to schedular
shecudelar . shecudlig in the pods acrosss multipe nodes   kubernetes deponds upon the constraints like memory etc
suppose if the application requires x constarints the scheduler will look the appromaite work node for that application

controller manager --- detects control cluster state changes
they are four
1.node point controller
2.repplication controller
3.end point controller

these controller are responsible for health check of entire cluster
it ensures the nodes are up and running
it ensure the correct number of pods are running as mention in the spec file

etcd: etcd is central data base store current state of the cluster at any point of time. it is  key value distributed data base the data will stored related to cluster
worker node components
kubelet : kubelet is the primary node agent in every worker node inside the cluster.kubelet interacts with the both container and node.kubelet start the pod with a 
container inside. And assign resources from node to the container.bascially it look the pod spec and submitted to api server in master
ensure that pod to be running and healthy.if the kubelet notices any issues in the pod in the worker node . then it will restart the pods in same worker node
if the problem within in the worker node itself then the matser detects the failure node and launches the pod in another worker node.
if all depends on pod is replication controller or repplication set

kube proxy . It is responsible for mantaining the entire network configuration and it also exposes the service to outside the world


every node have multiple pods associated with it
worker node do the actual work
Every node is unique is ip adddress . in the kubernetes when we deployed the pod in the cluster we get pod ip address



https://www.salesforceben.com/wp-content/uploads/2021/06/workflow3-768x476.png

At the start of the sprint, you create a release branch from the master/main branch.
Feature branches are created by each member of your team as they begin work on each user story.
You then build your changes in individual dev sandboxes (and/or IDE) and periodically commit them to the feature branch.
When your feature is ready for QA, you open a pull request and merge the feature branch into the QA integration branch.
After merging any change to QA integration, your branch gets deployed to the QA sandbox, where it can be tested and signed off by your QA team.
Once this is complete, a PR is opened to merge the feature branch into the Stage integration branch.
After merging any change to Stage integration, the branch is deployed to the Stage sandbox, where it can be tested and signed off for final release to production. Depending on your workflow, you might also have UAT by end users in there for good measure.
Once the change is approved in Stage, the feature branch is merged into the release branch. This process then repeats itself for every change made during the sprint, with each approved feature collected in the release branch.
At the end of the sprint, all changes in the release branch are deployed to production and set live for your end users.
Once youâ€™ve verified the new work on production, you merge the release branch back into master, so that master is once again up to date with production.
Youâ€™d then also want to merge master back into the integration branches, so that your branches stay in sync should any rogue changes have not followed this main process.



## kubernetes security with best practices
  
  Image scanning
  We are doing the image scaning by using synk.
  scaning during build in ci/cd pipeline. scan image before pushing to the repositorie
  scan in the repositories.
  
  Image scan will do check for vulnerabilities and check for misconfigurations
  
  Second security is run container as non user or non privallege usert
  disabling the root user access
  create a dedicate user and group in docker file
  FROM ubuntu
  RUN groupadd -r hanuman && useradd -r -g hanuman hanuman
  
  ###disabling the root user
  RUN chsh -s /usr/sbin/nologin root
  
  ENV HOME /home/hanuman
  
  ENV DEBIAN_FRONTEND=noninteractive
  
  docker run -it -u hanuman imagename /bin/bash
  docker run -it -u hanuman --security-opts=no-new-privileged imagename /bin/bash
  docker run -it --security-opts=no-new-privileged imagename /bin/bash
  
  privileged (user mode effective is zero (by pass all kernal permission checks ) it is super user
  unprivileged (full permisssion checking based on process cred
  
  change to non root user with USER directive
  and security context in the pod manifest file allow privalledge escaulation false
  
Third security is Rb
  restrict the namespace , create user , wha access  give to user  within namespace  then create users,role rolebinding in kubernetes
suppose if u create the role verbs are get nd resources are pod only then user can only acess list the pods within that namespace
if user restrict the namespace first u have to authencating creating key  and certificate by following below steps



User certificate creation


 Creating Users and Roles

Generating private key for John (john.key)

$ openssl genrsa -out john.key 2048
Generating certificate signing request (john.csr)

openssl req -new -key john.key -out john.csr -subj "/CN=john/O=finance"
Copy kubernetes ca certificate and key from the master node kmaster

If you used my vagrant provisioning scripts, the root password for all the nodes is "kubeadmin"

lxc file pull kmaster/etc/kubernetes/pki/ca.key /kuber
lxc file pull kmaster/etc/kubernetes/pki/ca.crt /kuber
Sign the certificate using certificate authority

openssl x509 -req -in john.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out john.crt -days 365

 then craete the kubeconfig file and add user,namespace, context name in contexts and add key and cerificate in kubeconfig
 
 sudo cp ~/.kube/config john.kubeconfig
 cat john.crt | base64 -w0 
 cat john.key | base64 -w0
 
 example of kubeconfig
  apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOREV5TkRBeE5Wb1hEVE16TURNeE1URXlOREF4TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTnBxCmRwVFkrbkFrSnZmQjYvM1l5Y3JRZWY0T2kyK2l5TjMzbVZsSlJrNjFlcmpnSzhFTHJ6Yk9MTE5GV1hhN0lKRUgKeGswMlZUME1iTnNLZ1IyQXlNQjkxVHJHTFZtZVdoQ01YSkJMZ1pSOGExaThRd3dBUTBYRDlETCtKUk9IaS9LYwpxRUVFcG4wZDdtbXNWcFlVRkhYU1NDaVpETzF4dFFBQWdvYTNxd3Y0VXIwemhPVnE5QVkxNlRlWnZkZmtFUFpmCnEzczRURk9aVnNOTzRKZm5HUkRnZWRNT3FjclBOZjM1a0VKWXIvM2d0eUNsdE9SaWdqVU1TY0dhWXo2dCtBdksKVXNXNTNWTGtCMHdTWGE1a1lSTUF3L0t1Nnp2SzNaL3Fmb0JBa04vd2JSUXVPT3RMMDE4OGxCdzN5b2RNbFo3Two2enYvL1Q3dEFXRmRwZFRZdDZzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZPMWZZS3dFYnJkQ2NITWhLRTRmNlNmbXJCSDFNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSkZKczNqekJJYUJreTN0dEVoYQplbGhnUjFUckJtaEtnaEI1K1NuUFB5OTdzZjg4b0dXemlIUWFyZmZrK1R5M1dQU1dOOGVtOUtPN1dPRVhQemhXCjRaTEN1QTMxY2NNbXRwWmtUWTZSWGNTWTNiUUZpQ0xTT0RqbUtXQXNZdWcvNEVzVFVucWtpMVBNajVXUDd2TmUKUHJsMVBUbWNSTFkxWlMyYlN6NlUwaTdhd3B3SjVmM1FmVTJQYlV1bFNGSDl0ZGEzZVVxK3JvMXRYcEtzS2xuOQpjMVQ0VEVWRlJBcWVubGRleml3TnA0V3Y4RE4zWlZmVlFiMzBTVnVkSk5COTQ0clc5VmdkVFdqdmpkb0lHV09xCkdidGhGT1QyMHlKTm9GZC91SUMydTNCM0NzekhuaXlqcmgxV3hqSG8rZDI3RzhnMFd5cmdZV3R0RXlkRHFSUCsKYnNRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://10.127.157.157:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: john
    namespace: finance
  name: john-kubernetes
current-context: john-kubernetes
kind: Config
preferences: {}
users:
- name:	john
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2VENDQWFVQ0ZHR0djRlUxNE1uWlhLdk9tYk16aXIzejdKUWJNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nak13TXpFMU1EUTBOak15V2hjTk1qUXdNekUwTURRMApOak15V2pBaE1RMHdDd1lEVlFRRERBUnFiMmh1TVJBd0RnWURWUVFLREFkbWFXNWhibU5sTUlJQklqQU5CZ2txCmhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBeUI4WE5KejNjb0UxVmhzN2FkNER2akpIcEdHZHU0Y3cKUWFjaGt4QnhtVDdxZTlZNGNkQlNnT0grdXF5RFY3aDh1SS81aUIzZUpWcGlEaDJoWUkwcjByYUI3L1k2K2RrYgovUmRjV2diYzl4TTNVdEVseTFQVTRvZHNTNHkrZzVZaldTYVZLa0ZNTmJJMXFCalgwYVF0VWlmWkVsaHRkQ3dUCkpRNlIvRlpQeHBNbThmZnV0UmhQTHJOMlhDUkhFWVdtdDVleXQvWGo2Z1RvYng2SEszV1hLTU1UNW44OExOM0kKU0p6dVRhY0o5bElVL1BWcFJkSmJPaUd0eEszMXlxNllRbTdjcFJsa28reEhpb3FqblVBRThUam1SUVRaTWFSUAp2YWhUa0tOb1M1VUQ1OXY0cFU1Wmx5MkRvY2dWSnVaSUJibktzN3dlR0VyNk9nTGJiS05ZTXdJREFRQUJNQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFDVmRaRjZ1Y09wMGFxRFFpRGFPM3NvbG1QS2prd2VmWjUxZzFrWkRhYk4KZnE2ZlZkRlhTQjZRMnI5OTZ4M0Q5NG8rcEkzZmhRdEdleWtHdU5qbFBhNkNnVzUxVlNEQUlYd2NaMWdOWmFRMApFY0VjMVc4Q2hTbjZ6WUtwZmNpZlphMjlvQWEzQWZLczlOSVZSWlV5cmVrbHl3TXBsM0QvMjF6b0FzeUw1c0RHClZZbGpTWHBtYTZiSEdhek5BVWVkdUxkNHhzSCtrZllpWnJQR1FyTS9WQyt6VkwxNHpPMEhjUE9LRWJVVnpIVnMKTzRzeVMvRXdtWHZiWTZNdUN4aTFUWGZpTWpDQjBZZjl1UFE0SmJBa01lSzFvdmRmc20wUmNjMVlaeWpkZFhDUwp2a3FhQUpSVzBQQTFsS0JTM2ZlT2NxOXJpYzN1R2IvOXdWcjhsYUdLTDd0VAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== 
    client-key-data: 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 

 create a role : kubectl create role john-finance  --verb=get,list --resource=pods --namespace finance
 
 roles are
 1.apigroups
2.verbs-get,list etc
3.resources-pods,deploy

role has be applied to namespace
note:clusterrole has be applied to cluster

rolebinding - user to binding to that particular role

kubectl create rolebinding john-finance-rolebinding1 --role=john-finance --user=john  --namespace finance


then user get the access of namespace
kubectl -n finance get pods
not the entire cluster

 
  4th security is Network policy : allow least access by applying rules
  
by default each pod has communicate with another pod
  using network policy restrict the access from one pod to another pod
  example:frontend pod can communicate with backend pod
  not the data base pod
  
  # example:
  
    network policy
  
  test
  We have deployed a new pod called secure-pod and a service called secure-service. Incoming or Outgoing connections to this pod are not working.
Troubleshoot why this is happening.

Make sure that incoming connection from the pod webapp-color are successful.

Important: Don't delete any current objects deployed.
  
  kubectl exec -it webapp-color -- sh
  nc -v -z -w 2 secure-service 80
  
  
apiVersion: v1
kind: List
items:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    creationTimestamp: "2023-02-07T10:29:32Z"
    generation: 1
    name: np
    namespace: default
    resourceVersion: "1930"
    uid: 717f3df9-eedd-4b8b-b93e-2c1e8ec85f03
  spec:
    podSelector:
      matchLabels:
        run: secure-pod

    policyTypes:
    - Ingress
    ingress:
    - from:
        
        - podSelector:
            matchLabels:
              name: webapp-color
              
      ports:
      - protocol: TCP          
        port: 80

  5. image pull secret from docker registry
  login to docker hub 
  docker login
  then ur cred are stored in below file
  cat /root/.docker/config.json
  kubectl create secret generic my-registry --from-file=.dockerconfigjson=.docker/config.json --type=kubernetes.io/dockerconfigjson
  apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      imagePullSecrets:
      - name: my-registry

      containers:
      - image: docker89781/dotnetappgithub
        name: nginx
        ports:
        - containerPort: 80
        resources: {}
        imagePullPolicy: Always
status: {}

  then u have to secret in imagePullSecrets in pod spec
  
  kube hunter . it will discover vulnerabilties and issue in kubernetes issues in the cluster. it will measure security issues in nodes in cluster and pod level
